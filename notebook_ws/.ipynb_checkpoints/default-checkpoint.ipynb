{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/construct_bannr.jpg\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/openclass_197_banner.jpg\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"bg-primary text-center\">\n",
    "    - Summary -\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3D Object Detection and Navigation** is a crucial skill in robotics and computer vision applications. It enables robots to **perceive and interact with their environment**, making it essential for tasks such as autonomous navigation, object manipulation, and environmental mapping. This technology has wide-ranging applications in fields like warehouse automation, search and rescue operations, and assistive robotics.\n",
    "\n",
    "In the upcoming Open Class, you'll how to implement 3D object detection using **Darknet** and **YOLO** and apply it to navigate the LIMO robot toward detected objects.\n",
    "\n",
    "What you'll learn:\n",
    "- Introduction to 3D Object Detection in Robotics\n",
    "- Understanding Darknet and YOLO architecture and its application in 3D space\n",
    "- Integrating Darknet and YOLO with ROS 2 for real-time object detection\n",
    "- Implementing navigation algorithms to move the LIMO robot toward detected objects\n",
    "- Hands-on experience in building a complete perception-action pipeline\n",
    "\n",
    "Robot Used: **LIMO Robot**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"bg-primary text-center\">\n",
    "    - Summary -\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 class=\"text-center\">\n",
    "        <span class=\"text-primary\">Introduction</span>\n",
    "        &nbsp;\n",
    "        <span class=\"\">What is 3D Object Detection in Robotics ? <br>\n",
    "        \n",
    "<img src=\"images/car_3d_detect.jpg\"  /></span>\n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3D **object detection** is a critical task in robotics, enabling robots to **perceive, understand, and interact with their environment** in a more advanced and autonomous manner. Unlike traditional 2D object detection, which identifies objects in images or video frames, 3D object detection involves **recognizing and localizing objects in three-dimensional space**. This process provides richer spatial information such as depth, orientation, and position, which is crucial for tasks like navigation, manipulation, and interaction in real-world environments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 3D Object Detection is Important in Robotics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Robots, especially those operating in dynamic environments, need to detect objects not just on a flat plane but in three-dimensional space to avoid collisions, manipulate objects, and interact with the world. 3D object detection enhances:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Autonomous driving**: Self-driving cars use 3D object detection to recognize vehicles, pedestrians, and obstacles, determining their exact location and speed.\n",
    "\n",
    "<img src=\"images/tesla_aug_vision.jpg\">\n",
    "\n",
    "This example Tesla's `Augmented Vision` debug. \n",
    "\n",
    "- **Robotic grasping**: In industrial robotics, 3D detection helps robots accurately locate objects for pick-and-place tasks, handling various objects regardless of their orientation.\n",
    "\n",
    "\n",
    "<img src=\"https://www.frontiersin.org/files/Articles/1038658/frobt-10-1038658-HTML/image_m/frobt-10-1038658-g002.jpg\" >\n",
    "\n",
    "- **Drone navigation**: UAVs (unmanned aerial vehicles) rely on 3D object detection to avoid obstacles, map terrain, and navigate safely.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/228941748/figure/fig5/AS:300872178388993@1448744925185/a-Dense-maximum-likelihood-occupancy-voxel-map-of-the-environment-depicted-in-Fig-5a.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How 3D Object Detection Works\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3D object detection involves several key components:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Sensors**: These provide the raw data, usually in the form of point clouds or depth maps. Commonly used sensors include:\n",
    "\n",
    "    - LiDAR (Light Detection and Ranging): Emits laser beams to measure distances and build detailed 3D point clouds.\n",
    "    - Stereo cameras: Capture two images from slightly different viewpoints, allowing depth calculation through triangulation.\n",
    "    - Depth cameras: Like Microsoft Kinect or Intel RealSense, they provide a depth map alongside standard RGB data.\n",
    "    - Point Clouds and 3D Data Representation: Point clouds are the most common representation for 3D object detection. A point cloud consists of many data points in a 3D coordinate system, where each point represents a specific location in space. Other representations include voxels (3D pixels) or meshes.\n",
    "    \n",
    "    **We will be using the depth camera included with the LIMO robot.**\n",
    "    \n",
    "    \n",
    "\n",
    "2. **Algorithms**: Once the 3D data is acquired, algorithms are used to identify and classify objects. Common approaches include:\n",
    "\n",
    "    - Convolutional Neural Networks (CNNs): Deep learning models that can be adapted for 3D data. These models take advantage of the spatial structure of objects and classify them in 3D.\n",
    "    - Region Proposal Networks (RPNs): Used to generate potential regions in the point cloud that might contain objects, followed by classification and refinement.\n",
    "    \n",
    "    **We will be using YOLOv7 to perform the object detection.**\n",
    "    \n",
    "\n",
    "3. **Bounding Boxes**: Detected objects are usually represented with 3D bounding boxes that provide information about the object's position, size, and orientation in the environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 class=\"text-center\">\n",
    "        <span class=\"text-primary\">YOLO</span>\n",
    "        &nbsp;\n",
    "        <span class=\"\">Understanding YOLO architecture and its application in 3D space <br>\n",
    "        \n",
    "<img src=\"https://user-images.githubusercontent.com/26833433/212094133-6bb8c21c-3d47-41df-a512-81c5931054ae.png\"  /></span>\n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLO, or *You Only Look Once*, is a real-time object detection algorithm known for its high-speed performance and accuracy. Unlike traditional object detection methods that apply sliding windows or region proposals over the entire image (e.g., R-CNN), YOLO reframes the problem as a single regression problem. It predicts bounding boxes and class probabilities directly from full images in a single pass through the network, hence the name.\n",
    "\n",
    "YOLO's architecture consists of:\n",
    "\n",
    "1.  **Input**: The model takes an image and divides it into an SÃ—SS \\times SSÃ—S grid. Each grid cell is responsible for predicting a certain number of bounding boxes and class probabilities for objects whose centers fall inside the cell.\n",
    "\n",
    "2.  **Bounding Boxes**: Each bounding box is represented by 5 predictions: (x,y,w,h,c)(x, y, w, h, c)(x,y,w,h,c), where:\n",
    "\n",
    "    -   x and y are the coordinates of the center of the box.\n",
    "    -   w and h are the width and height of the box.\n",
    "    -   c is the confidence score that reflects how likely it is that the box contains an object.\n",
    "3.  **Class Probabilities**: For each grid cell, YOLO predicts the probability distribution over all the object classes. These probabilities are conditioned on the box containing an object.\n",
    "\n",
    "4.  **Backbone Network**: YOLO uses a convolutional neural network (CNN) as a feature extractor (such as Darknet). Early versions of YOLO used a simpler CNN, while more advanced versions like YOLOv3 and YOLOv5 use deeper networks, such as ResNet or CSPDarknet53, for improved accuracy and feature extraction.\n",
    "\n",
    "5.  **Loss Function**: The loss function used in YOLO combines several components, including:\n",
    "\n",
    "    -   Localization loss: Penalizes errors in bounding box coordinates.\n",
    "    -   Confidence loss: Penalizes incorrect object/no-object classification.\n",
    "    -   Classification loss: Penalizes incorrect predictions of class labels.\n",
    "        \n",
    "        \n",
    "#### Applications of YOLO in 3D Object Detection:\n",
    "\n",
    "-   **Autonomous Vehicles**: Detecting pedestrians, vehicles, cyclists, and other objects in 3D space, crucial for safe navigation.\n",
    "-   **Robotics**: Detecting and understanding the 3D positions of objects in the environment for tasks like manipulation or interaction.\n",
    "-   **Augmented Reality (AR)**: Integrating virtual objects with real-world 3D scenes by understanding object positions and dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 class=\"text-center\">\n",
    "        <span class=\"text-primary\">Start Simulation</span>        \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided gazebo Limo simulation will help you throughout the lesson. \n",
    "\n",
    "Use it to test your algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-play\"></i>\n",
    "    &nbsp;\n",
    "    Execute in Terminal #1\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros2 launch limo_description limo_basic.launch.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/open_sim.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/sim.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue;\">Get one LIMO PRO Robot and practice ROS 2!</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\">\n",
    "    <tr>\n",
    "        <td><h1>Limo PRO includes:</h1><br>\n",
    "            <ul><h1>Lidar</h1></ul>\n",
    "            <ul><h1>IMU</h1></ul>\n",
    "            <ul><h1>RGB-D camera ORBEC Dabai</h1></ul>\n",
    "            <ul><h1>Nvidia Jetson Orin nano</h1></ul>\n",
    "            <ul><h1>7'' display</h1></ul>\n",
    "        </td>\n",
    "        <td><img src=\"images/limo_gif.gif\" width=\"300\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue;\">Get yours here: https://app.theconstruct.ai/product/agile-x-limo-pro-robot-for-education-and-research</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 class=\"text-center\">\n",
    "        <span class=\"text-primary\">Debug View</span>\n",
    "        &nbsp;\n",
    "        <span class=\"\">What are we working with ? <br>\n",
    "        \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already opened your workspace you will see that some of the important packages like [darknet](https://github.com/AlexeyAB/darknet) and [darknet_ros](https://github.com/ros2/openrobotics_darknet_ros?tab=readme-ov-file) are already setup for you. \n",
    "\n",
    "If you would like to set it up on on your own rosject feel free to follow the documentation on [https://github.com/ros2/openrobotics_darknet_ros](https://github.com/ros2/openrobotics_darknet_ros?tab=readme-ov-file) but for the purposes of this openclass everything is provided for you, no installs necessary. \n",
    "\n",
    "Since we have briefly gone into what YOLO family of models does letâ€™s hit the ground running and launch it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-play\"></i>\n",
    "    &nbsp;\n",
    "    Execute in Terminal #2\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/ros2_ws\n",
    "ros2 launch openrobotics_darknet_ros detector_launch.py rgb_image:=/camera/image_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And â€¦. nothing ? \n",
    "\n",
    "Not exactly, this package doesnâ€™t launch with a debug view out of the box which is why we will be creating our own. \n",
    "\n",
    "However if you do a ros2 topic list you can see that it is publishing on a topics.\n",
    "\n",
    "So lets create our own debug view. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-play\"></i>\n",
    "    &nbsp;\n",
    "    Execute in Terminal #3\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/ros2_ws/src\n",
    "ros2 pkg create --build-type ament_python 3d_object_detection --dependencies rclpy sensor_msgs vision_msgs cv_bridge tf2_ros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will need to edit a few files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-file\"></i>\n",
    "    &nbsp;\n",
    "    setup.py\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setuptools import find_packages, setup\n",
    "\n",
    "package_name = '3d_object_detection'\n",
    "\n",
    "setup(\n",
    "    name=package_name,\n",
    "    version='0.0.0',\n",
    "    packages=find_packages(exclude=['test']),\n",
    "    data_files=[\n",
    "        ('share/ament_index/resource_index/packages',\n",
    "            ['resource/' + package_name]),\n",
    "        ('share/' + package_name, ['package.xml']),\n",
    "    ],\n",
    "    install_requires=['setuptools'],\n",
    "    zip_safe=True,\n",
    "    maintainer='user',\n",
    "    maintainer_email='user@todo.todo',\n",
    "    description='TODO: Package description',\n",
    "    license='TODO: License declaration',\n",
    "    tests_require=['pytest'],\n",
    "    entry_points={\n",
    "        'console_scripts': [\n",
    "            '3d_object_detection_node = 3d_object_detection.3d_object_detection_node:main'\n",
    "        ],\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you have to create the main python script. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/create_pkg.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-file\"></i>\n",
    "    &nbsp;\n",
    "    3d_object_detection_node.py\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rclpy\n",
    "from rclpy.node import Node\n",
    "from sensor_msgs.msg import Image\n",
    "from vision_msgs.msg import Detection2DArray\n",
    "from cv_bridge import CvBridge, CvBridgeError\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class ObjectDetectionOverlay(Node):\n",
    "    def __init__(self):\n",
    "        super().__init__('object_detection_overlay')\n",
    "\n",
    "        # Subscribe to the camera image and detection topics\n",
    "        self.image_sub = self.create_subscription(\n",
    "            Image, '/camera/depth/image_raw', self.image_callback, 10)\n",
    "        self.detections_sub = self.create_subscription(\n",
    "            Detection2DArray, '/detector_node/detections', self.detection_callback, 10)\n",
    "\n",
    "        # Initialize other variables\n",
    "        self.bridge = CvBridge()\n",
    "        self.detections = None\n",
    "        self.image = None\n",
    "\n",
    "    def image_callback(self, msg):\n",
    "        try:\n",
    "            # Convert the depth image to a grayscale 8-bit image for visualization\n",
    "            depth_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='32FC1')\n",
    "            # Normalize the depth image to range 0-255 for visualization\n",
    "            normalized_depth_image = cv2.normalize(depth_image, None, 0, 255, cv2.NORM_MINMAX)\n",
    "            # Convert to 8-bit grayscale\n",
    "            self.image = np.uint8(normalized_depth_image)\n",
    "\n",
    "            # Convert grayscale to color to allow color drawings (bounding boxes)\n",
    "            self.image = cv2.cvtColor(self.image, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "            # Process and display the image with detections overlayed\n",
    "            if self.detections is not None:\n",
    "                self.overlay_detections()\n",
    "        except CvBridgeError as e:\n",
    "            self.get_logger().error(f\"Error converting image: {str(e)}\")\n",
    "\n",
    "    def detection_callback(self, msg):\n",
    "        # Store the latest detections\n",
    "        self.detections = msg\n",
    "\n",
    "    def overlay_detections(self):\n",
    "        if self.image is None:\n",
    "            return\n",
    "\n",
    "        # Create a copy of the image to draw on\n",
    "        overlay_image = self.image.copy()\n",
    "\n",
    "        for detection in self.detections.detections:\n",
    "            bbox = detection.bbox\n",
    "            results = detection.results[0]  # Assuming only one result per detection\n",
    "\n",
    "            # Calculate the bounding box in pixel coordinates\n",
    "            x_min = int(bbox.center.position.x - (bbox.size_x / 2))\n",
    "            y_min = int(bbox.center.position.y - (bbox.size_y / 2))\n",
    "            x_max = int(bbox.center.position.x + (bbox.size_x / 2))\n",
    "            y_max = int(bbox.center.position.y + (bbox.size_y / 2))\n",
    "\n",
    "            # Draw the bounding box\n",
    "            cv2.rectangle(overlay_image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "\n",
    "            # Overlay the detected object's name and confidence level\n",
    "            object_name = results.hypothesis.class_id\n",
    "            confidence = results.hypothesis.score * 100  # Confidence in percentage\n",
    "            label = f\"{object_name}: {confidence:.2f}%\"\n",
    "\n",
    "            # Put the label above the bounding box\n",
    "            cv2.putText(overlay_image, label, (x_min, y_min - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Show the image with OpenCV\n",
    "        cv2.imshow(\"Detections Overlay\", overlay_image)\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "def main(args=None):\n",
    "    rclpy.init(args=args)\n",
    "    node = ObjectDetectionOverlay()\n",
    "    rclpy.spin(node)\n",
    "    node.destroy_node()\n",
    "    rclpy.shutdown()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can build and run your new package. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-play\"></i>\n",
    "    &nbsp;\n",
    "    Execute in Terminal #3\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/ros2_ws\n",
    "colcon build --packages-select 3d_object_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source install/setup.bash\n",
    "ros2 run 3d_object_detection 3d_object_detection_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have to drive a little to the objects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-play\"></i>\n",
    "    &nbsp;\n",
    "    Execute in Terminal\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros2 run teleop_twist_keyboard teleop_twist_keyboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then when you are infront of the objects you should get. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/stage_one.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've done:\n",
    "\n",
    "- **Image Processing**: The depth image is converted and prepared for visualization (grayscale to color).\n",
    "- **Detection Handling**: The detection_callback stores new detections as they arrive.\n",
    "- **Detection Overlay**: The stored detections are drawn onto the image in the form of bounding boxes and labels, and the final image with the overlay is displayed in real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Click for Detailed breakdown</b> ðŸ‘‡ </summary>\n",
    "<br>\n",
    "    \n",
    "### 1\\. **Image Callback** (`image_callback`)\n",
    "\n",
    "-   This callback is triggered whenever a new image is received from the camera. The goal here is to process the image for visualization and overlay detections.\n",
    "\n",
    "-   **Image Conversion**: It uses `CvBridge` to convert the ROS image message (`sensor_msgs/Image`) into an OpenCV image. In this case, it's converting a depth image (32-bit float) into a format that can be visualized by normalizing it and converting it to an 8-bit grayscale image.\n",
    "\n",
    "-   **Image Preparation**: The grayscale image is converted to a color (BGR) format, allowing further operations such as drawing colored bounding boxes on the image.\n",
    "\n",
    "-   **Overlay Detections**: If object detections are available (i.e., received through the other callback), it calls the `overlay_detections` method, which adds the detected objects' bounding boxes and labels to the image.\n",
    "\n",
    "### 2\\. **Detection Callback** (`detection_callback`)\n",
    "\n",
    "-   This callback is triggered whenever new detection data is received from the object detection topic.\n",
    "\n",
    "-   **Store Detections**: The detection data (`Detection2DArray`) is stored in the `self.detections` variable. This detection data contains bounding boxes, object class labels, and confidence scores.\n",
    "\n",
    "-   This detection data is used later in the `overlay_detections` method to draw the detected objects onto the image.\n",
    "\n",
    "### 3\\. **Overlaying Detections** (`overlay_detections`)\n",
    "\n",
    "-   This method processes the stored detection data and overlays it on the image.\n",
    "\n",
    "-   **Bounding Box Calculation**: For each detection, the 2D bounding box coordinates are calculated using the detection's center position (`bbox.center.position.x`, `bbox.center.position.y`) and size (`bbox.size_x`, `bbox.size_y`). The top-left and bottom-right corners of the bounding box are derived from this.\n",
    "\n",
    "-   **Drawing Bounding Boxes**: A green bounding box is drawn on the image around each detected object using OpenCV's `rectangle` function.\n",
    "\n",
    "-   **Label and Confidence**: The detected object's class name (`class_id`) and confidence score (`hypothesis.score`) are also extracted. This information is overlaid above the bounding box in text form, showing the object's label and how confident the model is about the detection.\n",
    "\n",
    "-   **Display Image**: After overlaying the bounding boxes and labels, the updated image is displayed using OpenCV's `imshow` function. This visualizes the detected objects in the camera's view.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 class=\"text-center\">\n",
    "        <span class=\"text-primary\">What is that ?</span>\n",
    "        &nbsp;\n",
    "        <span class=\"\">Dealing with model confidence <br>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see now the model is really outputting predictions. \n",
    "\n",
    "But whatâ€™s this ? aeroplane ??? \n",
    "\n",
    "See if you look at the confidence next to it you can see that the random detection has a confidence of 71.75 percent while the person which we know actually exists has 85.91, so lets filter out our detections and make sure only detections with at least 80% confidence gets considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_callback(self, msg):\n",
    "    # Filter detections based on confidence\n",
    "    filtered_detections = []\n",
    "    for detection in msg.detections:\n",
    "        # Assuming the first result contains the hypothesis with the highest confidence\n",
    "        if detection.results[0].hypothesis.score >= 0.80:  # 80% confidence threshold\n",
    "            filtered_detections.append(detection)\n",
    "\n",
    "    # Store the filtered detections\n",
    "    self.detections = Detection2DArray(detections=filtered_detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to rebuild our package using the previous command and rerun. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/stage_2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 class=\"text-center\">\n",
    "        <span class=\"text-primary\">So, where exactly is it ?</span>\n",
    "        &nbsp;\n",
    "        <span class=\"\">Extrapolating 3D Position <br>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this is great and all but isnâ€™t this class about 3d object detection ? \n",
    "\n",
    "Yep ! \n",
    "\n",
    "And thats why we overlayed out detections on the depth image and not the color, because now we will be using the depth image to get the 3d position of the object. \n",
    "\n",
    "Here are the steps we are going to follow. \n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Calculate the centroid of the bounding box** from the detection.\n",
    "2. **Get the depth value** from the depth image at the centroid.\n",
    "3. **Convert 2D image coordinates (u, v) and depth (z)** into 3D space using the camera's intrinsic parameters (from the `/camera/depth/camera_info` topic).\n",
    "4. Display the 3D coordinates in the OpenCV window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-file\"></i>\n",
    "    &nbsp;\n",
    "    3d_object_detection_node.py\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rclpy\n",
    "from rclpy.node import Node\n",
    "from sensor_msgs.msg import Image, CameraInfo\n",
    "from vision_msgs.msg import Detection2DArray\n",
    "from cv_bridge import CvBridge, CvBridgeError\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class ObjectDetectionOverlay(Node):\n",
    "    def __init__(self):\n",
    "        super().__init__('object_detection_overlay')\n",
    "\n",
    "        # Subscribe to the camera image, camera info, and detection topics\n",
    "        self.image_sub = self.create_subscription(\n",
    "            Image, '/camera/depth/image_raw', self.image_callback, 10)\n",
    "        self.camera_info_sub = self.create_subscription(\n",
    "            CameraInfo, '/camera/depth/camera_info', self.camera_info_callback, 10)\n",
    "        self.detections_sub = self.create_subscription(\n",
    "            Detection2DArray, '/detector_node/detections', self.detection_callback, 10)\n",
    "\n",
    "        # Initialize other variables\n",
    "        self.bridge = CvBridge()\n",
    "        self.detections = None\n",
    "        self.image = None\n",
    "        self.depth_image = None  # Original depth image for 3D calculations\n",
    "        self.camera_info = None  # Camera intrinsic parameters\n",
    "\n",
    "    def camera_info_callback(self, msg):\n",
    "        # Store the camera info for intrinsic parameters\n",
    "        self.camera_info = msg\n",
    "\n",
    "    def image_callback(self, msg):\n",
    "        try:\n",
    "            # Convert the depth image to a float32 format\n",
    "            depth_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='32FC1')\n",
    "            \n",
    "            # Normalize the depth image to range 0-255 for visualization (optional)\n",
    "            normalized_depth_image = cv2.normalize(depth_image, None, 0, 255, cv2.NORM_MINMAX)\n",
    "            \n",
    "            # Convert normalized depth image to 8-bit grayscale for displaying\n",
    "            display_image = np.uint8(normalized_depth_image)\n",
    "\n",
    "            # Convert grayscale to color to allow color drawings (bounding boxes)\n",
    "            display_image = cv2.cvtColor(display_image, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "            # Store the images\n",
    "            self.depth_image = depth_image  # Original depth image for 3D calculations\n",
    "            self.image = display_image  # Image for visualization\n",
    "\n",
    "            # Process and display the image with detections overlayed\n",
    "            if self.detections is not None:\n",
    "                self.overlay_detections()\n",
    "\n",
    "        except CvBridgeError as e:\n",
    "            self.get_logger().error(f\"Error converting image: {str(e)}\")\n",
    "\n",
    "    def detection_callback(self, msg):\n",
    "        # Filter detections based on confidence\n",
    "        filtered_detections = []\n",
    "        for detection in msg.detections:\n",
    "            # Assuming the first result contains the hypothesis with the highest confidence\n",
    "            if detection.results[0].hypothesis.score >= 0.80:  # 80% confidence threshold\n",
    "                filtered_detections.append(detection)\n",
    "\n",
    "        # Store the filtered detections\n",
    "        self.detections = Detection2DArray(detections=filtered_detections)\n",
    "\n",
    "    def overlay_detections(self):\n",
    "        if self.image is None or self.camera_info is None or self.depth_image is None:\n",
    "            return\n",
    "\n",
    "        # Get the intrinsic parameters from camera_info\n",
    "        fx = self.camera_info.k[0]  # Focal length in x\n",
    "        fy = self.camera_info.k[4]  # Focal length in y\n",
    "        cx = self.camera_info.k[2]  # Optical center in x\n",
    "        cy = self.camera_info.k[5]  # Optical center in y\n",
    "\n",
    "        # Create a copy of the image to draw on\n",
    "        overlay_image = self.image.copy()\n",
    "\n",
    "        for detection in self.detections.detections:\n",
    "            bbox = detection.bbox\n",
    "            results = detection.results[0]  # Assuming only one result per detection\n",
    "\n",
    "            # Calculate the bounding box in pixel coordinates\n",
    "            x_min = int(bbox.center.position.x - (bbox.size_x / 2))\n",
    "            y_min = int(bbox.center.position.y - (bbox.size_y / 2))\n",
    "            x_max = int(bbox.center.position.x + (bbox.size_x / 2))\n",
    "            y_max = int(bbox.center.position.y + (bbox.size_y / 2))\n",
    "\n",
    "            # Draw the bounding box\n",
    "            cv2.rectangle(overlay_image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "\n",
    "            # Calculate the centroid of the bounding box\n",
    "            centroid_x = int(bbox.center.position.x)\n",
    "            centroid_y = int(bbox.center.position.y)\n",
    "\n",
    "            # Draw the centroid\n",
    "            cv2.circle(overlay_image, (centroid_x, centroid_y), 5, (0, 0, 255), -1)\n",
    "\n",
    "            # Get the depth at the centroid from the original depth image\n",
    "            depth_value = self.depth_image[centroid_y, centroid_x]  # Use depth_image, not display_image\n",
    "\n",
    "            # If depth is valid (not NaN or infinity)\n",
    "            if np.isscalar(depth_value) and not np.isnan(depth_value) and not np.isinf(depth_value):\n",
    "                # Calculate the 3D position in the camera_depth_optical_frame\n",
    "                z = depth_value\n",
    "                x = (centroid_x - cx) * z / fx\n",
    "                y = (centroid_y - cy) * z / fy\n",
    "\n",
    "                # Overlay the 3D position on the image\n",
    "                label_3d = f\"X: {x:.2f}, Y: {y:.2f}, Z: {z:.2f} m\"\n",
    "                cv2.putText(overlay_image, label_3d, (x_min, y_max + 20),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)\n",
    "\n",
    "            # Overlay the detected object's name and confidence level\n",
    "            object_name = results.hypothesis.class_id\n",
    "            confidence = results.hypothesis.score * 100  # Confidence in percentage\n",
    "            label = f\"{object_name}: {confidence:.2f}%\"\n",
    "\n",
    "            # Put the label above the bounding box\n",
    "            cv2.putText(overlay_image, label, (x_min, y_min - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Show the image with OpenCV\n",
    "        cv2.imshow(\"Detections Overlay\", overlay_image)\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "def main(args=None):\n",
    "    rclpy.init(args=args)\n",
    "    node = ObjectDetectionOverlay()\n",
    "    rclpy.spin(node)\n",
    "    node.destroy_node()\n",
    "    rclpy.shutdown()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can rebuild and rerun. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/stage_3.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we:\n",
    "    \n",
    "-   **Camera Info Subscription**: The node now listens to the camera info topic to retrieve intrinsic parameters for calculating 3D positions.\n",
    "-   **Depth Image Storage**: The raw depth image is stored separately for accurate 3D calculations.\n",
    "-   **Confidence Filtering**: Detections are filtered based on a confidence threshold (80%) to only visualize reliable detections.\n",
    "-   **3D Position Calculation**: For each detected object, the 3D position in camera space is calculated using depth data and camera intrinsics.\n",
    "-   **3D Position Overlay**: The 3D position is displayed on the image, giving real-world coordinates (X, Y, Z) for each detected object.\n",
    "-   **Centroid Marking**: A red circle is drawn at the centroid of each bounding box, which is used for depth retrieval and 3D calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Click for Detailed breakdown</b> ðŸ‘‡ </summary>\n",
    "<br>\n",
    "\n",
    "### 1\\. **Camera Info Subscription (`camera_info_callback`)**\n",
    "\n",
    "-   The code now subscribes to the `/camera/depth/camera_info` topic, which provides camera intrinsic parameters (like focal lengths and optical center). This information is critical for calculating 3D positions from 2D pixel coordinates and depth values.\n",
    "-   **Intrinsic Parameters**: The `camera_info_callback` stores the intrinsic parameters of the camera in `self.camera_info`. These values are later used to convert 2D bounding box coordinates into 3D positions using depth information.\n",
    "\n",
    "### 2\\. **Depth Image Storage**\n",
    "\n",
    "-   The raw depth image is now stored separately in `self.depth_image`, which is essential for calculating 3D object positions. The `image_callback` still processes the image for visualization (converting it to grayscale, normalizing, and displaying), but the original depth data is preserved for use in 3D calculations.\n",
    "\n",
    "### 3\\. **Detection Filtering in `detection_callback`**\n",
    "\n",
    "-   **Filtering Detections by Confidence**: The code now filters detections based on a confidence threshold. Only detections with a confidence score of 80% or higher are stored and used for overlay.\n",
    "-   This step ensures that only reliable detections are visualized and processed, which helps reduce noise from low-confidence predictions.\n",
    "\n",
    "### 4\\. **3D Position Calculation and Overlay**\n",
    "\n",
    "-   **3D Position Calculation**: The main addition is the calculation of the 3D position of each detected object. For each bounding box, the centroid is found, and the depth value at the centroid is retrieved from the original depth image (`self.depth_image`).\n",
    "\n",
    "    -   The retrieved depth value represents the distance from the camera to the object in meters.\n",
    "    -   Using the intrinsic parameters from `self.camera_info` (focal lengths and optical center), the code calculates the 3D coordinates (x,y,z)(x, y, z)(x,y,z) of the object in the camera frame using the following formulas:\n",
    "        -   zzz is the depth value at the centroid.\n",
    "        -   x=(u-cx)Ã—z/fxx = (u - cx) \\times z / fxx=(u-cx)Ã—z/fx, where uuu is the x-coordinate of the centroid.\n",
    "        -   y=(v-cy)Ã—z/fyy = (v - cy) \\times z / fyy=(v-cy)Ã—z/fy, where vvv is the y-coordinate of the centroid.\n",
    "    -   These 3D coordinates represent the object's position in the camera's optical frame (in meters).\n",
    "-   **3D Position Display**: The 3D coordinates are overlayed on the image in the form of a text label below the bounding box. This label shows the object's position relative to the camera in terms of X, Y, and Z coordinates (in meters), giving spatial awareness.\n",
    "\n",
    "### 5\\. **Centroid Overlay**\n",
    "\n",
    "-   A red circle is drawn at the centroid of each detected bounding box, helping to visually mark the center of the object in the image. The centroid is also used to retrieve the depth value for 3D calculations.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 class=\"text-center\">\n",
    "        <span class=\"text-primary\">Rossification</span>\n",
    "        &nbsp;\n",
    "        <span class=\"\">Publishing a TF <br>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now while this is great for us, its not really useful for doing anything within the robot because this data that we are extrapolating is only within this openCV window. \n",
    "\n",
    "Therefore lets publish this data.\n",
    "\n",
    "Now its possible to create a new topics and stream the data that way but a much better way is to publish this position as a transform to the camera frame (TF). This means we can then later use it for navigation among other means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-file\"></i>\n",
    "    &nbsp;\n",
    "    3d_object_detection_node.py\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rclpy\n",
    "from rclpy.node import Node\n",
    "from sensor_msgs.msg import Image, CameraInfo\n",
    "from vision_msgs.msg import Detection2DArray\n",
    "from cv_bridge import CvBridge, CvBridgeError\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tf2_ros\n",
    "import geometry_msgs.msg\n",
    "\n",
    "class ObjectDetectionOverlay(Node):\n",
    "    def __init__(self):\n",
    "        super().__init__('object_detection_overlay')\n",
    "\n",
    "        # Subscribe to the camera image, camera info, and detection topics\n",
    "        self.image_sub = self.create_subscription(\n",
    "            Image, '/camera/depth/image_raw', self.image_callback, 10)\n",
    "        self.camera_info_sub = self.create_subscription(\n",
    "            CameraInfo, '/camera/depth/camera_info', self.camera_info_callback, 10)\n",
    "        self.detections_sub = self.create_subscription(\n",
    "            Detection2DArray, '/detector_node/detections', self.detection_callback, 10)\n",
    "\n",
    "        # Initialize TF broadcaster\n",
    "        self.tf_broadcaster = tf2_ros.TransformBroadcaster(self)\n",
    "\n",
    "        # Initialize other variables\n",
    "        self.bridge = CvBridge()\n",
    "        self.detections = None\n",
    "        self.image = None\n",
    "        self.depth_image = None  # Original depth image for 3D calculations\n",
    "        self.camera_info = None  # Camera intrinsic parameters\n",
    "\n",
    "    def camera_info_callback(self, msg):\n",
    "        # Store the camera info for intrinsic parameters\n",
    "        self.camera_info = msg\n",
    "\n",
    "    def image_callback(self, msg):\n",
    "        try:\n",
    "            # Convert the depth image to a float32 format\n",
    "            depth_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='32FC1')\n",
    "            \n",
    "            # Normalize the depth image to range 0-255 for visualization (optional)\n",
    "            normalized_depth_image = cv2.normalize(depth_image, None, 0, 255, cv2.NORM_MINMAX)\n",
    "            \n",
    "            # Convert normalized depth image to 8-bit grayscale for displaying\n",
    "            display_image = np.uint8(normalized_depth_image)\n",
    "\n",
    "            # Convert grayscale to color to allow color drawings (bounding boxes)\n",
    "            display_image = cv2.cvtColor(display_image, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "            # Store the images\n",
    "            self.depth_image = depth_image  # Original depth image for 3D calculations\n",
    "            self.image = display_image  # Image for visualization\n",
    "\n",
    "            # Process and display the image with detections overlayed\n",
    "            if self.detections is not None:\n",
    "                self.overlay_detections()\n",
    "\n",
    "        except CvBridgeError as e:\n",
    "            self.get_logger().error(f\"Error converting image: {str(e)}\")\n",
    "\n",
    "    def detection_callback(self, msg):\n",
    "        # Filter detections based on confidence\n",
    "        filtered_detections = []\n",
    "        for detection in msg.detections:\n",
    "            # Assuming the first result contains the hypothesis with the highest confidence\n",
    "            if detection.results[0].hypothesis.score >= 0.80:  # 80% confidence threshold\n",
    "                filtered_detections.append(detection)\n",
    "\n",
    "        # Store the filtered detections\n",
    "        self.detections = Detection2DArray(detections=filtered_detections)\n",
    "\n",
    "    def overlay_detections(self):\n",
    "        if self.image is None or self.camera_info is None or self.depth_image is None:\n",
    "            return\n",
    "\n",
    "        # Get the intrinsic parameters from camera_info\n",
    "        fx = self.camera_info.k[0]  # Focal length in x\n",
    "        fy = self.camera_info.k[4]  # Focal length in y\n",
    "        cx = self.camera_info.k[2]  # Optical center in x\n",
    "        cy = self.camera_info.k[5]  # Optical center in y\n",
    "\n",
    "        # Create a copy of the image to draw on\n",
    "        overlay_image = self.image.copy()\n",
    "\n",
    "        for detection in self.detections.detections:\n",
    "            bbox = detection.bbox\n",
    "            results = detection.results[0]  # Assuming only one result per detection\n",
    "\n",
    "            # Calculate the bounding box in pixel coordinates\n",
    "            x_min = int(bbox.center.position.x - (bbox.size_x / 2))\n",
    "            y_min = int(bbox.center.position.y - (bbox.size_y / 2))\n",
    "            x_max = int(bbox.center.position.x + (bbox.size_x / 2))\n",
    "            y_max = int(bbox.center.position.y + (bbox.size_y / 2))\n",
    "\n",
    "            # Draw the bounding box\n",
    "            cv2.rectangle(overlay_image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "\n",
    "            # Calculate the centroid of the bounding box\n",
    "            centroid_x = int(bbox.center.position.x)\n",
    "            centroid_y = int(bbox.center.position.y)\n",
    "\n",
    "            # Draw the centroid\n",
    "            cv2.circle(overlay_image, (centroid_x, centroid_y), 5, (0, 0, 255), -1)\n",
    "\n",
    "            # Get the depth at the centroid from the original depth image\n",
    "            depth_value = self.depth_image[centroid_y, centroid_x]  # Use depth_image, not display_image\n",
    "\n",
    "            # If depth is valid (not NaN or infinity)\n",
    "            if np.isscalar(depth_value) and not np.isnan(depth_value) and not np.isinf(depth_value):\n",
    "                # Calculate the 3D position in the camera_depth_optical_frame\n",
    "                z = depth_value\n",
    "                x = (centroid_x - cx) * z / fx\n",
    "                y = (centroid_y - cy) * z / fy\n",
    "\n",
    "                # Publish TF at the detected object's position\n",
    "                self.publish_tf(x, y, z, results.hypothesis.class_id)\n",
    "\n",
    "                # Overlay the 3D position on the image\n",
    "                label_3d = f\"X: {x:.2f}, Y: {y:.2f}, Z: {z:.2f} m\"\n",
    "                cv2.putText(overlay_image, label_3d, (x_min, y_max + 20),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)\n",
    "\n",
    "            # Overlay the detected object's name and confidence level\n",
    "            object_name = results.hypothesis.class_id\n",
    "            confidence = results.hypothesis.score * 100  # Confidence in percentage\n",
    "            label = f\"{object_name}: {confidence:.2f}%\"\n",
    "\n",
    "            # Put the label above the bounding box\n",
    "            cv2.putText(overlay_image, label, (x_min, y_min - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        # Show the image with OpenCV\n",
    "        cv2.imshow(\"Detections Overlay\", overlay_image)\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "    def publish_tf(self, x, y, z, object_name):\n",
    "        # Create a TransformStamped message\n",
    "        t = geometry_msgs.msg.TransformStamped()\n",
    "        t.header.stamp = self.get_clock().now().to_msg()\n",
    "        t.header.frame_id = \"camera_depth_optical_frame\"  # Parent frame\n",
    "        t.child_frame_id = f\"{object_name}_frame\"  # Child frame for the detected object\n",
    "        t.transform.translation.x = float(x)\n",
    "        t.transform.translation.y = float(y)\n",
    "        t.transform.translation.z = float(z)\n",
    "        t.transform.rotation.x = 0.0\n",
    "        t.transform.rotation.y = 0.0\n",
    "        t.transform.rotation.z = 0.0\n",
    "        t.transform.rotation.w = 1.0  # Default rotation (no rotation)\n",
    "\n",
    "        # Publish the transform\n",
    "        self.tf_broadcaster.sendTransform(t)\n",
    "\n",
    "def main(args=None):\n",
    "    rclpy.init(args=args)\n",
    "    node = ObjectDetectionOverlay()\n",
    "    rclpy.spin(node)\n",
    "    node.destroy_node()\n",
    "    rclpy.shutdown()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/final_stage.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in this section we:\n",
    "    \n",
    "-   **TF Broadcasting**: The 3D position of each detected object is published as a transform using `tf2_ros.TransformBroadcaster`. This allows other ROS nodes to subscribe to the object's pose in real-time.\n",
    "-   **Dynamic Frame Creation**: The frame for each detected object is dynamically named based on the object's class (e.g., `\"person_frame\"`, `\"car_frame\"`), making it easy to track individual objects.\n",
    "-   **Integration with Object Detection**: The 3D position of each detected object is still displayed on the image, but it is now also available to the broader ROS network through the TF system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Click for Detailed breakdown</b> ðŸ‘‡ </summary>\n",
    "<br>\n",
    "\n",
    "\n",
    "### 1\\. **TF Broadcaster Initialization**\n",
    "\n",
    "-   The node now initializes a `tf2_ros.TransformBroadcaster` object (`self.tf_broadcaster`), which is used to broadcast the 3D position of detected objects as a transform in ROS.\n",
    "-   **TF Broadcaster**: It allows other nodes to receive real-time transformations between frames, which is useful for tasks like tracking objects in the environment.\n",
    "\n",
    "### 2\\. **3D Position Broadcasting via `publish_tf`**\n",
    "\n",
    "-   The new method `publish_tf(self, x, y, z, object_name)` is responsible for publishing the 3D position of the detected object as a transform.\n",
    "-   **TransformStamped Message**: This is the message type used to broadcast transformations in ROS. It includes the translation (`x`, `y`, `z`) of the object relative to the camera frame, as well as a rotation (which is set to no rotation in this case).\n",
    "-   **Parent and Child Frames**:\n",
    "    -   The parent frame is set to `\"camera_depth_optical_frame\"`, representing the camera's depth frame.\n",
    "    -   The child frame is dynamically named using the `object_name`, such as `\"person_frame\"`, `\"car_frame\"`, etc., representing the detected object's own frame.\n",
    "-   **Publishing the Transform**: The transform is published using `self.tf_broadcaster.sendTransform(t)`, which allows other ROS nodes to access the 3D position of the object in the environment.\n",
    "\n",
    "### 3\\. **Enhancements in 3D Position Calculation and Display**\n",
    "\n",
    "-   After calculating the 3D coordinates (`x`, `y`, `z`) of the detected object, the code now calls `self.publish_tf()` to publish the object's 3D position as a transform.\n",
    "-   The 3D position is also displayed on the image as a text label, showing the real-world X, Y, and Z coordinates in meters.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 class=\"text-center\">\n",
    "        <span class=\"text-primary\">Navigate !</span>\n",
    "        &nbsp;\n",
    "        <span class=\"\">Navigation using TF <br>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to navigate to detected objects lets create a new package that will make use of our new transform value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-play\"></i>\n",
    "    &nbsp;\n",
    "    Execute in Terminal #4\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros2 pkg create --build-type ament_python move_towards_object --dependencies rclpy geometry_msgs tf2_ros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit setup.py to define your package information. It should look like this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-file\"></i>\n",
    "    &nbsp;\n",
    "    setup.py\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setuptools import find_packages, setup\n",
    "\n",
    "package_name = 'move_towards_object'\n",
    "\n",
    "setup(\n",
    "    name=package_name,\n",
    "    version='0.0.0',\n",
    "    packages=find_packages(exclude=['test']),\n",
    "    data_files=[\n",
    "        ('share/ament_index/resource_index/packages',\n",
    "            ['resource/' + package_name]),\n",
    "        ('share/' + package_name, ['package.xml']),\n",
    "    ],\n",
    "    install_requires=['setuptools'],\n",
    "    zip_safe=True,\n",
    "    maintainer='user',\n",
    "    maintainer_email='user@todo.todo',\n",
    "    description='TODO: Package description',\n",
    "    license='TODO: License declaration',\n",
    "    tests_require=['pytest'],\n",
    "    entry_points={\n",
    "        'console_scripts': [\n",
    "            'move_towards_object_node = move_towards_object.move_towards_object_node:main',\n",
    "        ],\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create a new script. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/add_file_2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-file\"></i>\n",
    "    &nbsp;\n",
    "    move_towards_object_node.py\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rclpy\n",
    "from rclpy.node import Node\n",
    "import tf2_ros\n",
    "import geometry_msgs.msg\n",
    "from geometry_msgs.msg import Twist\n",
    "import math\n",
    "\n",
    "class MoveTowardsObject(Node):\n",
    "    def __init__(self):\n",
    "        super().__init__('move_towards_object')\n",
    "\n",
    "        # Create a publisher for velocity commands\n",
    "        self.velocity_publisher = self.create_publisher(Twist, '/cmd_vel', 10)\n",
    "\n",
    "        # Create a TF buffer and listener\n",
    "        self.tf_buffer = tf2_ros.Buffer()\n",
    "        self.tf_listener = tf2_ros.TransformListener(self.tf_buffer, self)\n",
    "\n",
    "        # Timer to periodically check the transform and move the robot\n",
    "        self.timer = self.create_timer(0.1, self.control_loop)\n",
    "\n",
    "        # Define the target frame that you want to move towards\n",
    "        self.target_frame = 'person_frame'  # Replace with the actual TF child frame name\n",
    "\n",
    "    def control_loop(self):\n",
    "        try:\n",
    "            # Look up the latest transform from the target object to the camera frame\n",
    "            trans = self.tf_buffer.lookup_transform('camera_depth_optical_frame', self.target_frame, rclpy.time.Time())\n",
    "\n",
    "            # Extract the translation components (x, y, z)\n",
    "            x = trans.transform.translation.x\n",
    "            y = trans.transform.translation.y\n",
    "\n",
    "            # Calculate distance and angle to the target\n",
    "            distance = math.sqrt(x**2 + y**2)\n",
    "            angle_to_target = math.atan2(y, x)\n",
    "\n",
    "            # Define a Twist message to move the robot\n",
    "            twist = Twist()\n",
    "\n",
    "            print(\"[DEBUG] Angle to target: \", angle_to_target)\n",
    "            print(\"[DEBUG] Distance to target: \", distance)\n",
    "\n",
    "            # Move forward if the target is not too close\n",
    "            if distance > 0.04:  # Stop if we're within 0.5 meters of the target\n",
    "                twist.linear.x = 0.2 * distance  # Scale speed by distance\n",
    "                twist.angular.z = 0.1 * angle_to_target  # Turn towards the target\n",
    "            else:\n",
    "                print(\"Too close to person !\")\n",
    "                twist.linear.x = 0.0\n",
    "                twist.angular.z = 0.0\n",
    "\n",
    "            # Publish the Twist message to move the robot\n",
    "            self.velocity_publisher.publish(twist)\n",
    "\n",
    "        except tf2_ros.LookupException:\n",
    "            self.get_logger().warn(f\"Could not find transform from camera_depth_optical_frame to {self.target_frame}\")\n",
    "        except tf2_ros.ExtrapolationException as e:\n",
    "            self.get_logger().warn(f\"Extrapolation error: {str(e)}\")\n",
    "\n",
    "def main(args=None):\n",
    "    rclpy.init(args=args)\n",
    "    node = MoveTowardsObject()\n",
    "    rclpy.spin(node)\n",
    "    node.destroy_node()\n",
    "    rclpy.shutdown()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build your package and run !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"badge badge-pill badge-primary\">\n",
    "    <i class=\"fa fa-play\"></i>\n",
    "    &nbsp;\n",
    "    Execute in Terminal #4\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/ros2_ws\n",
    "colcon build --packages-select move_towards_object\n",
    "source install/setup.bash\n",
    "ros2 run move_towards_object move_towards_object_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/final.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Components\n",
    "\n",
    "1. **Node Initialization**:\n",
    "    \n",
    "    - The `MoveTowardsObject` class inherits from `Node` and initializes the node named `move_towards_object`.\n",
    "    \n",
    "2. **Publisher for Velocity Commands**:\n",
    "    \n",
    "    - A publisher (`self.velocity_publisher`) is created to send velocity commands (`Twist` messages) to the `/cmd_vel` topic, which controls the robot's movement.\n",
    "    \n",
    "3. **TF Buffer and Listener**:\n",
    "    \n",
    "    - A `tf2_ros.Buffer` and a `tf2_ros.TransformListener` are initialized. This allows the node to listen for transformations between coordinate frames in ROS, specifically from the target frame (e.g., the detected object) to the camera frame.\n",
    "    \n",
    "4. **Timer for Control Loop**:\n",
    "    \n",
    "    - A timer is set up to call the `control_loop` method every 0.1 seconds. This method continuously checks the robot's position relative to the detected object.\n",
    "    \n",
    "5. **Target Frame**:\n",
    "    \n",
    "    - The target frame is defined as `person_frame`, which is the frame for the detected object that the robot will move towards.\n",
    "\n",
    "### Control Loop Logic\n",
    "\n",
    "In the `control_loop` method, the following steps occur:\n",
    "\n",
    "1. **Transform Lookup**:\n",
    "    \n",
    "    - The method attempts to look up the latest transform from the camera frame (`camera_depth_optical_frame`) to the target frame (`person_frame`). If the transform is not available, it handles exceptions and logs warnings.\n",
    "    \n",
    "2. **Distance and Angle Calculation**:\n",
    "    \n",
    "    - If the transform is successfully retrieved, the translation components (x, y) are extracted.\n",
    "    \n",
    "    - The distance to the target object is calculated using the Euclidean distance formula: distance = sqrt(x^2 + y^2).\n",
    "        \n",
    "    - The angle to the target is calculated using the `atan2` function: angle_to_target = math.atan2(y, x).\n",
    "\n",
    "3. **Movement Command**:\n",
    "    \n",
    "    - A `Twist` message is created to define the robot's linear and angular velocities.\n",
    "    \n",
    "    - If the distance to the target is greater than 0.04 meters, the robot moves towards the target:\n",
    "        \n",
    "        - The linear velocity is scaled by the distance to maintain proportional movement: twist.linear.x = 0.2 * distance.\n",
    "        \n",
    "        - The angular velocity is adjusted to turn towards the target: twist.angular.z = 0.1 * angle_to_target.\n",
    "\n",
    "    - If the robot is too close to the detected object (less than or equal to 0.04 meters), it stops moving.\n",
    "\n",
    "4. **Publishing Velocity Commands**:\n",
    "    \n",
    "    - The calculated `Twist` message is published to the `/cmd_vel` topic, commanding the robot to move or stop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <h1 class=\"text-center\">\n",
    "        <span class=\"\">Go Further !</span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This course will embark on an exciting journey into the realm of perception in robotics using ROS 2. Through a series of structured units and hands-on projects, you will explore various aspects of sensor data processing and perception techniques.\n",
    "\n",
    "#### What You Will Learn\n",
    "\n",
    "ROS 2, Perception, Image Processing, OpenCV, Point Cloud Porcessing, Yolo, Advanced Perception Techniques, Deep Learning and more!\n",
    "    \n",
    "<a href=\"https://app.theconstruct.ai/courses/239\"><img src=\"images/ros2-percep.png\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Share your work ðŸ“¨\n",
    "\n",
    "We love seeing what our students do with the things they learn here so please do share what you do on social media and make sure to tag us so we know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Twitter (X) | @_TheConstruct_](https://twitter.com/_theconstruct_) ![](data:image/png;base64,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)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Facebook | The Construct ](https://www.facebook.com/theconstructsim) ![](data:image/png;base64,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)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
